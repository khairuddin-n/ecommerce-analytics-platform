version: '3.8'

services:
  # PostgreSQL for Airflow metadata
  postgres:
    image: postgres:13-alpine
    container_name: ecommerce_postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5
    networks:
      - ecommerce-network

  # Airflow Webserver
  airflow-webserver:
    build:
      context: .
      dockerfile: docker/Dockerfile.airflow
    container_name: ecommerce_airflow_webserver
    depends_on:
      postgres:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      PYTHONPATH: /opt/airflow
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./src:/opt/airflow/src
      - ./data:/opt/airflow/data
      - ./config:/opt/airflow/config
    ports:
      - "8080:8080"
    command: webserver
    networks:
      - ecommerce-network

  # Airflow Scheduler
  airflow-scheduler:
    build:
      context: .
      dockerfile: docker/Dockerfile.airflow
    container_name: ecommerce_airflow_scheduler
    depends_on:
      postgres:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      PYTHONPATH: /opt/airflow
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./src:/opt/airflow/src
      - ./data:/opt/airflow/data
      - ./config:/opt/airflow/config
    command: scheduler
    networks:
      - ecommerce-network

  # Airflow Init
  airflow-init:
    build:
      context: .
      dockerfile: docker/Dockerfile.airflow
    container_name: ecommerce_airflow_init
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__FERNET_KEY: ''
    command: >
      bash -c "
        airflow db init &&
        airflow users create \
          --username admin \
          --password admin \
          --firstname Admin \
          --lastname User \
          --role Admin \
          --email admin@example.com
      "
    networks:
      - ecommerce-network

  # Spark Master (Optional - for distributed processing)
  spark-master:
    build:
      context: .
      dockerfile: docker/Dockerfile.spark
    container_name: ecommerce_spark_master
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8081
    ports:
      - "8081:8081"
      - "7077:7077"
    volumes:
      - ./src:/opt/spark-apps
      - ./data:/opt/spark-data
    networks:
      - ecommerce-network

  # Spark Worker
  spark-worker:
    build:
      context: .
      dockerfile: docker/Dockerfile.spark
    container_name: ecommerce_spark_worker
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
    volumes:
      - ./src:/opt/spark-apps
      - ./data:/opt/spark-data
    networks:
      - ecommerce-network

  # Analytics application service (production)
  analytics:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: ecommerce_analytics_app
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
      - ./.env:/app/.env
    environment:
      - PYTHONUNBUFFERED=1
      - SPARK_LOCAL_IP=127.0.0.1
      - ENV=${ENV:-development}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    networks:
      - ecommerce-network
    command: python -m src.pipeline.main

  # Development service with hot reload
  analytics-dev:
    build:
      context: .
      dockerfile: docker/Dockerfile.dev
    container_name: ecommerce_analytics_dev
    volumes:
      - ./src:/app/src
      - ./tests:/app/tests
      - ./scripts:/app/scripts
      - ./data:/app/data
      - ./logs:/app/logs
      - ./.env:/app/.env
      - ./dags:/app/dags
    environment:
      - PYTHONUNBUFFERED=1
      - SPARK_LOCAL_IP=127.0.0.1
      - ENV=development
      - LOG_LEVEL=DEBUG
    networks:
      - ecommerce-network
    stdin_open: true
    tty: true

  # Jupyter notebook for data exploration
  jupyter:
    build:
      context: .
      dockerfile: docker/Dockerfile.dev
    container_name: ecommerce_jupyter
    volumes:
      - ./src:/app/src
      - ./data:/app/data
      - ./notebooks:/app/notebooks
      - ./.env:/app/.env
    environment:
      - PYTHONPATH=/app
    ports:
      - "8888:8888"
    networks:
      - ecommerce-network
    command: jupyter notebook --ip=0.0.0.0 --port=8888 --no-browser --allow-root --NotebookApp.token=''

volumes:
  postgres-db-volume:

networks:
  ecommerce-network:
    driver: bridge